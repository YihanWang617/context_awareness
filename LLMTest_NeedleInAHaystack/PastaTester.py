from typing import Optional
from HuggingfaceTester import HuggingfaceTester
import numpy as np
import torch
import os
from typing import List
from jsonargparse import CLI
import asyncio
from pastalib.pasta import PASTA
import sys
sys.path.append('../')
from UserSteeredWrapper import UserSteeredWrapper

class PastaTester(HuggingfaceTester):
    
    # PASTA_HEAD_CONFIGS = {
    #     "tinyllama": {k: None for k in range(22)},
    #     "llama-2": {k: None for k in range(32)},
    #     "llama-3": {k: None for k in range(32)},
    # }


    # PASTA_HEAD_CONFIGS = {
    #     "tinyllama": {k: None for k in range(22)},
    #     "llama-2-7b": {0: [20, 23, 2, 22, 11, 13, 24, 30, 7, 21], 1: [28, 23, 20, 27, 17, 11, 18, 26, 8, 9], 2: [6, 8, 13, 15, 3, 23, 25, 26, 22, 21], 3: [23, 21, 28, 27, 0, 26, 9, 8, 17, 31], 4: [24, 12, 26, 14, 20, 8, 21, 2, 5, 1], 5: [6, 2, 21, 12, 4, 13, 20, 23, 1, 3], 6: [7, 9, 29, 2, 25, 18, 13, 21, 16, 17], 7: [28, 21, 29, 13, 0, 14, 20, 12, 6, 10], 8: [7, 29, 23, 31, 24, 3, 22, 26, 5, 25], 9: [26, 22, 7, 31, 0, 20, 6, 21, 12, 30], 10: [31, 8, 11, 24, 0, 18, 23, 19, 4, 29], 11: [7, 24, 28, 31, 30, 19, 17, 9, 15, 14], 12: [8, 1, 22, 9, 10, 13, 16, 14, 11, 4], 13: [3, 22, 8, 17, 4, 18, 2, 9, 23, 30], 14: [19, 15, 9, 1, 2, 29, 7, 3, 24, 18], 15: [8, 31, 9, 17, 19, 24, 27, 5, 10, 14], 16: [2, 14, 4, 20, 13, 3, 18, 5, 30, 19], 17: [2, 1, 11, 0, 10, 9, 18, 13, 22, 30], 18: [1, 5, 16, 31, 15, 10, 23, 12, 9, 30], 19: [28, 31, 13, 14, 10, 8, 4, 19, 6, 15], 20: [23, 10, 0, 16, 12, 5, 11, 3, 30, 29], 21: [15, 31, 9, 1, 4, 27, 10, 16, 13, 30], 22: [8, 3, 31, 11, 19, 30, 16, 27, 23, 22], 23: [2, 25, 5, 29, 28, 17, 6, 20, 8, 31], 24: [25, 24, 3, 16, 15, 4, 1, 30, 11, 17], 25: [2, 16, 30, 5, 4, 21, 0, 28, 3, 11], 26: [3, 26, 15, 21, 24, 25, 10, 14, 19, 27], 27: [14, 10, 1, 23, 25, 16, 4, 2, 22, 7], 28: [11, 8, 18, 25, 14, 17, 4, 13, 9, 7], 29: [15, 2, 28, 5, 17, 19, 9, 10, 18, 13], 30: [27, 25, 12, 19, 4, 26, 15, 14, 9, 29], 31: [29, 17, 16, 6, 12, 25, 2, 24, 15, 22]},
    #     "llama-2-13b": {0: [2, 22, 0, 4, 9, 6, 7, 1, 10, 20], 1: [17, 34, 36, 8, 15, 31, 19, 35, 30, 26], 2: [24, 4, 7, 23, 30, 36, 38, 19, 16, 9], 3: [23, 0, 5, 39, 22, 14, 8, 15, 24, 20], 4: [31, 32, 10, 35, 13, 9, 5, 1, 20, 33], 5: [28, 21, 18, 24, 39, 37, 17, 3, 33, 27], 6: [31, 6, 30, 22, 37, 36, 12, 23, 20, 39], 7: [23, 14, 3, 28, 33, 36, 10, 12, 15, 4], 8: [9, 22, 38, 16, 17, 33, 24, 20, 4, 35], 9: [36, 3, 20, 21, 37, 22, 28, 33, 34, 6], 10: [30, 29, 6, 5, 9, 24, 25, 23, 31, 18], 11: [21, 17, 4, 12, 1, 25, 32, 35, 5, 37], 12: [7, 8, 24, 36, 21, 1, 31, 18, 16, 5], 13: [14, 4, 36, 17, 12, 13, 11, 21, 24, 23], 14: [8, 25, 1, 39, 33, 13, 0, 23, 4, 20], 15: [24, 6, 34, 15, 4, 17, 20, 25, 7, 39], 16: [9, 25, 11, 21, 1, 31, 33, 30, 3, 2], 17: [21, 30, 35, 6, 16, 4, 19, 3, 5, 2], 18: [2, 18, 34, 37, 22, 19, 16, 6, 35, 14], 19: [5, 16, 29, 8, 1, 27, 0, 33, 3, 4], 20: [11, 8, 36, 18, 0, 1, 24, 25, 23, 34], 21: [4, 27, 39, 22, 37, 25, 14, 1, 6, 5], 22: [27, 16, 34, 2, 8, 29, 5, 33, 6, 1], 23: [18, 26, 33, 15, 1, 9, 38, 0, 23, 13], 24: [19, 4, 29, 21, 2, 17, 25, 24, 26, 16], 25: [0, 21, 25, 26, 30, 33, 36, 9, 18, 4], 26: [21, 36, 7, 22, 24, 30, 15, 5, 29, 13], 27: [39, 29, 12, 5, 14, 30, 18, 1, 33, 36], 28: [21, 38, 31, 10, 8, 17, 5, 9, 20, 22], 29: [33, 20, 15, 34, 28, 24, 4, 2, 22, 31], 30: [35, 6, 1, 28, 10, 38, 36, 5, 24, 14], 31: [6, 37, 39, 2, 18, 9, 36, 21, 1, 27], 32: [27, 24, 4, 14, 12, 32, 33, 35, 19, 25], 33: [30, 11, 23, 12, 7, 0, 34, 25, 24, 18], 34: [21, 27, 34, 38, 7, 10, 23, 8, 32, 2], 35: [21, 6, 9, 38, 24, 34, 26, 17, 30, 4], 36: [30, 21, 27, 32, 23, 29, 3, 16, 2, 20], 37: [30, 25, 5, 24, 15, 0, 16, 21, 4, 8], 38: [16, 39, 7, 2, 29, 13, 37, 9, 28, 12], 39: [37, 25, 33, 1, 6, 10, 19, 24, 29, 8]},
    #     "llama-3": {0: [12, 20, 9, 13, 8, 28, 15, 21, 22, 30], 1: [23, 12, 10, 22, 2, 31, 9, 29, 28, 8], 2: [22, 28, 21, 10, 5, 4, 6, 19, 14, 15], 3: [2, 28, 8, 23, 20, 29, 9, 22, 21, 0], 4: [0, 27, 5, 15, 22, 1, 10, 28, 19, 16], 5: [11, 25, 27, 17, 18, 5, 1, 24, 7, 4], 6: [31, 28, 14, 26, 20, 24, 2, 9, 6, 25], 7: [21, 6, 7, 13, 10, 15, 24, 25, 22, 26], 8: [24, 29, 2, 10, 31, 18, 8, 1, 9, 11], 9: [0, 16, 31, 2, 27, 3, 29, 17, 12, 19], 10: [7, 14, 2, 28, 3, 1, 19, 31, 18, 13], 11: [24, 15, 3, 13, 5, 2, 7, 9, 6, 18], 12: [11, 26, 28, 1, 8, 23, 31, 15, 9, 10], 13: [14, 6, 13, 2, 17, 3, 4, 8, 5, 18], 14: [3, 6, 29, 10, 12, 2, 22, 30, 5, 31], 15: [25, 8, 26, 4, 23, 5, 11, 21, 29, 30], 16: [24, 6, 8, 28, 19, 20, 26, 0, 25, 1], 17: [15, 14, 8, 31, 28, 13, 7, 24, 27, 29], 18: [29, 0, 9, 28, 14, 24, 2, 22, 8, 20], 19: [18, 10, 15, 2, 23, 9, 20, 3, 14, 13], 20: [3, 30, 0, 25, 23, 12, 24, 1, 14, 26], 21: [21, 23, 3, 11, 26, 18, 1, 8, 2, 31], 22: [28, 15, 27, 4, 11, 12, 5, 8, 14, 29], 23: [2, 24, 20, 7, 18, 27, 0, 1, 3, 22], 24: [7, 18, 17, 24, 14, 5, 12, 13, 4, 27], 25: [9, 17, 6, 1, 3, 19, 18, 0, 2, 5], 26: [23, 4, 2, 27, 3, 7, 30, 6, 17, 19], 27: 6, 11, 14, 21, 30, 28, 13, 16, 5, 7], 28: [16, 11, 10, 15, 30, 0, 20, 21, 18, 23], 29: [11, 28, 22, 8, 9, 23, 30, 20, 29, 16], 30: [26, 18, 2, 17, 20, 6, 1, 0, 9, 27], 31: [22, 16, 2, 20, 27, 30, 28, 7, 29, 21]},
    # }

    # PASTA_HEAD_CONFIGS = {
    #     "tinyllama": {k: None for k in range(22)},
    #     "llama-2-7b": {0: [13, 24, 30, 7, 21], 1: [11, 18, 26, 8, 9], 2: [23, 25, 26, 22, 21], 3: [26, 9, 8, 17, 31], 4: [8, 21, 2, 5, 1], 5: [13, 20, 23, 1, 3], 6: [18, 13, 21, 16, 17], 7: [14, 20, 12, 6, 10], 8: [3, 22, 26, 5, 25], 9: [20, 6, 21, 12, 30], 10: [18, 23, 19, 4, 29], 11: [19, 17, 9, 15, 14], 12: [13, 16, 14, 11, 4], 13: [18, 2, 9, 23, 30], 14: [29, 7, 3, 24, 18], 15: [24, 27, 5, 10, 14], 16: [3, 18, 5, 30, 19], 17: [9, 18, 13, 22, 30], 18: [10, 23, 12, 9, 30], 19: [8, 4, 19, 6, 15], 20: [5, 11, 3, 30, 29], 21: [27, 10, 16, 13, 30], 22: [30, 16, 27, 23, 22], 23: [17, 6, 20, 8, 31], 24: [4, 1, 30, 11, 17], 25: [21, 0, 28, 3, 11], 26: [25, 10, 14, 19, 27], 27: [6, 4, 2, 22, 7], 28: [17, 4, 13, 9, 7], 29: [19, 9, 10, 18, 13], 30: [26, 15, 14, 9, 29], 31: [25, 2, 24, 15, 22]},
    #     "llama-2-13b": {0: [6, 7, 1, 10, 20], 1: [31, 19, 35, 30, 26], 2: [36, 38, 19, 16, 9], 3: [14, 8, 15, 24, 20], 4: [9, 5, 1, 20, 33], 5: [37, 17, 3, 33, 27], 6: [36, 12, 23, 20, 39], 7: [36, 10, 12, 15, 4], 8: [33, 24, 20, 4, 35], 9: [22, 28, 33, 34, 6], 10: [24, 25, 23, 31, 18], 11: [25, 32, 35, 5, 37], 12: [1, 31, 18, 16, 5], 13: [ 13, 11, 21, 24, 23], 14: [13, 0, 23, 4, 20], 15: [17, 20, 25, 7, 39], 16: [31, 33, 30, 3, 2], 17: [4, 19, 3, 5, 2], 18: [19, 16, 6, 35, 14], 19: [27, 0, 33, 3, 4], 20: [1, 24, 25, 23, 34], 21: [25, 14, 1, 6, 5], 22: [29, 5, 33, 6, 1], 23: [9, 38, 0, 23, 13], 24: [17, 25, 24, 26, 16], 25: [33, 36, 9, 18, 4], 26: [30, 15, 5, 29, 13], 27: [30, 18, 1, 33, 36], 28: [17, 5, 9, 20, 22], 29: [24, 4, 2, 22, 31], 30: [38, 36, 5, 24, 14], 31: [9, 36, 21, 1, 27], 32: [32, 33, 35, 19, 25], 33: [0, 34, 25, 24, 18], 34: [10, 23, 8, 32, 2], 35: [34, 26, 17, 30, 4], 36: [29, 3, 16, 2, 20], 37: [0, 16, 21, 4, 8], 38: [13, 37, 9, 28, 12], 39: [10, 19, 24, 29, 8]},
    #     "llama-3": {0: [28, 15, 21, 22, 30], 1: [31, 9, 29, 28, 8], 2: [4, 6, 19, 14, 15], 3: [29, 9, 22, 21, 0], 4: [1, 10, 28, 19, 16], 5: [5, 1, 24, 7, 4], 6: [24, 2, 9, 6, 25], 7: [15, 24, 25, 22, 26], 8: [18, 8, 1, 9, 11], 9: [3, 29, 17, 12, 19], 10: [1, 19, 31, 18, 13], 11: [2, 7, 9, 6, 18], 12: [23, 31, 15, 9, 10], 13: [3, 4, 8, 5, 18], 14: [2, 22, 30, 5, 31], 15: [5, 11, 21, 29, 30], 16: [20, 26, 0, 25, 1], 17: [13, 7, 24, 27, 29], 18: [24, 2, 22, 8, 20], 19: [9, 20, 3, 14, 13], 20: [12, 24, 1, 14, 26], 21: [18, 1, 8, 2, 31], 22: [12, 5, 8, 14, 29], 23: [27, 0, 1, 3, 22], 24: [5, 12, 13, 4, 27], 25: [19, 18, 0, 2, 5], 26: [7, 30, 6, 17, 19], 27: [28, 13, 16, 5, 7], 28: [0, 20, 21, 18, 23], 29: [23, 30, 20, 29, 16], 30: [6, 1, 0, 9, 27], 31: [30, 28, 7, 29, 21]},
    # }

    PASTA_HEAD_CONFIGS = {
        "tinyllama": {k: None for k in range(22)},
        "llama-2-7b": {0: [21], 1: [9], 2: [21], 3: [31], 4: [1], 5: [3], 6: [17], 7: [10], 8: [25], 9: [30], 10: [29], 11: [14], 12: [4], 13: [30], 14: [18], 15: [14], 16: [19], 17: [30], 18: [30], 19: [15], 20: [29], 21: [30], 22: [22], 23: [31], 24: [17], 25: [11], 26: [27], 27: [7], 28: [7], 29: [13], 30: [29], 31: [22]},
        "llama-2-13b": {0: [20], 1: [26], 2: [9], 3: [20], 4: [33], 5: [27], 6: [39], 7: [4], 8: [35], 9: [6], 10: [18], 11: [37], 12: [5], 13: [23], 14: [20], 15: [39], 16: [2], 17: [2], 18: [14], 19: [4], 20: [34], 21: [5], 22: [1], 23: [13], 24: [16], 25: [4], 26: [13], 27: [36], 28: [22], 29: [31], 30: [14], 31: [27], 32: [25], 33: [18], 34: [2], 35: [4], 36: [20], 37: [8], 38: [12], 39: [8]},
        "llama-3": {0: [30], 1: [8], 2: [15], 3: [0], 4: [16], 5: [4], 6: [25], 7: [26], 8: [11], 9: [19], 10: [13], 11: [18], 12: [10], 13: [18], 14: [31], 15: [30], 16: [1], 17: [29], 18: [20], 19: [13], 20: [26], 21: [31], 22: [29], 23: [22], 24: [27], 25: [5], 26: [19], 27: [7], 28: [23], 29: [16], 30: [27], 31: [21]},
    }
    
    @staticmethod
    def get_pasta_head_config(model_name):
        for k, v in PastaTester.PASTA_HEAD_CONFIGS.items():
            if k in model_name.lower():
                return v
        return None

    def __init__(self, **kwargs):
        self.alpha = kwargs.pop("alpha")
        assert (self.alpha <= 1) and (self.alpha > 0)
        super().__init__(**kwargs)
        head_config = self.get_pasta_head_config(self.model_name)
        if head_config is None:
            raise ValueError(f"Pasta head config not found for {self.model_name}. Populate PASTA_HEAD_CONFIGS with retrieval heads for this model before running HayStack with Pasta intervention.")
        self.wrapped_model = UserSteeredWrapper(head_config, self.model_to_test, self.tokenizer, self.alpha)

    def generate(self, conversation_history, context=None, max_new_tokens=100):
        # print(prompt)
        
        output = self.wrapped_model.generate(conversation_history, context, max_new_tokens=max_new_tokens)
        
        return output

    async def get_response_from_model(self, prompt_context):
        # generate_ids = self.model_to_test.generate(inputs.input_ids, max_new_tokens=60, pad_token_id=self.tokenizer.pad_token_id)
        # generate_ids = generate_ids[:, inputs["input_ids"].shape[1]:]
        prompt, context = prompt_context
        generate_ids = self.generate(prompt, context, max_new_tokens=100)
        response = self.tokenizer.decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)

        return response

    def get_prompt(self, context, return_context=True):
        prompt, context = super().get_prompt(context, return_context=True)
        if return_context:
            return prompt, context
        else:
            return prompt

def main(model_name: str=None, template: str='default', needle_name: str="SF", evaluation_method: str="substring_match", 
         alpha: float=0.01, context_lengths_min: int=200, context_lengths_max: int=4000, start_context_lengths=200,
         context_lengths: Optional[List[int]]=None, document_depths: Optional[List[int]]=None,
         add_hint: bool=False, save_model_suffix: Optional[str]=None):

    assert model_name is not None
    assert template in ["raw", "default"]

    if save_model_suffix is None:
        save_model_suffix = str(alpha)

    tester = PastaTester(model_name=model_name, 
                         template=template,
                         evaluation_method=evaluation_method, 
                         alpha=alpha, 
                         context_lengths_min=context_lengths_min, 
                         context_lengths_max=context_lengths_max,
                         start_context_lengths=start_context_lengths,
                         context_lengths=context_lengths,
                         document_depth_percents=document_depths,
                         needle_name=needle_name,
                         add_hint=add_hint,
                         save_model_suffix=save_model_suffix,
                         attn_implementation='eager')
    tester.start_test()

if __name__ == '__main__':
    CLI(main)
