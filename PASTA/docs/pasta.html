<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>pastalib.pasta API documentation</title>
<meta name="description" content="PASTA Implementation" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pastalib.pasta</code></h1>
</header>
<section id="section-intro">
<p>PASTA Implementation</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;PASTA Implementation&#34;&#34;&#34;
import torch
import abc, json
from contextlib import contextmanager
from functools import partial
from pathlib import Path
from typing import Any, Literal, Optional, Sequence, cast, overload, Tuple

import transformers 
from pastalib.utils import tokenizer_utils
from pastalib.utils.typing import (
    Model,
    Dataset,
    Device,
    ModelInput,
    ModelOutput,
    StrSequence,
    Tokenizer,
    TokenizerOffsetMapping,
)


class PASTA(abc.ABC):

    ATTN_MODULE_NAME = {
        &#34;gptj&#34;: &#34;transformer.h.{}.attn&#34;,
        &#34;llama&#34;: &#34;model.layers.{}.self_attn&#34;,
    }
    ATTENTION_MASK_ARGIDX = {
        &#34;gptj&#34;: 2, 
        &#34;llama&#34;: 1, 
    }

    def __init__(
        self, 
        model: Model, 
        tokenizer: Tokenizer, 
        head_config: dict|list|None = None, 
        alpha: float = 0.01, 
        scale_position: str = &#34;exclude&#34;, 
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.setup_model(model)

        self.alpha = alpha
        self.scale_position = scale_position
        self.setup_head_config(head_config)

        assert self.scale_position in [&#39;include&#39;, &#39;exclude&#39;]
        assert self.alpha &gt; 0

    def setup_model(self, model):
        if isinstance(model, transformers.LlamaForCausalLM):
            self.model_name = &#34;llama&#34;
            self.num_attn_head = model.config.num_attention_heads
        elif isinstance(model, transformers.GPTJForCausalLM):
            self.model_name = &#34;gptj&#34;
            self.num_attn_head = model.config.n_head
        else:
            raise ValueError(&#34;Unimplemented Model Type.&#34;)

    def setup_head_config(self, head_config):
        if isinstance(head_config, dict):
            self.head_config = {int(k):v for k,v in head_config.items()} 
            self.all_layers_idx = [int(key) for key in head_config]
        elif isinstance(head_config, list):
            self.all_layers_idx = [int(v) for v in head_config]
            self.head_config = {
                idx:list(range(self.num_attn_head)) for idx in self.all_layers_idx
            }
        else:
            raise ValueError(f&#34;Incorrect head config: {head_config}&#34;)
    
    def _maybe_batch(self, text: str | StrSequence) -&gt; StrSequence:
        &#34;&#34;&#34;Batch the text if it is not already batched.&#34;&#34;&#34;
        if isinstance(text, str):
            return [text]
        return text

    def token_ranges_from_batch(
        self,
        strings: str | StrSequence,
        substrings: str | StrSequence,
        offsets_mapping: Sequence[TokenizerOffsetMapping],
        occurrence: int = 0,
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;Return shape (batch_size, 2) tensor of token ranges for (str, substr) pairs.&#34;&#34;&#34;
        strings = self._maybe_batch(strings)
        substrings = self._maybe_batch(substrings)
        if len(strings) != len(substrings):
            raise ValueError(
                f&#34;got {len(strings)} strings but only {len(substrings)} substrings&#34;
            )
        return torch.tensor(
            [
                tokenizer_utils.find_token_range(
                    string, substring, offset_mapping=offset_mapping, occurrence=occurrence
                )
                for string, substring, offset_mapping in zip(
                    strings, substrings, offsets_mapping
                )
            ]
        )

    def edit_attention_mask(
        self, 
        module: torch.nn.Module, 
        input_args: tuple,
        input_kwargs: dict, 
        head_idx: list[int],
        token_range: torch.Tensor, 
        input_len: int, 
    ):
        if &#34;attention_mask&#34; in input_kwargs:
            attention_mask = input_kwargs[&#39;attention_mask&#39;].clone()
        elif input_args is not None:
            arg_idx = self.ATTENTION_MASK_ARGIDX[self.model_name]
            attention_mask = input_args[arg_idx].clone()
        else:
            raise ValueError(f&#34;Not found attention masks in {str(module)}&#34;)
        
        bsz, head_dim, tgt_len, src_len = attention_mask.size()
        dtype, device = attention_mask.dtype, attention_mask.device
        if head_dim != self.num_attn_head:
            attention_mask = attention_mask.expand(
                bsz, self.num_attn_head, tgt_len, src_len
            ).clone()
        scale_constant = torch.Tensor([self.alpha]).to(dtype).to(device).log()
        
        for bi, (ti,tj) in enumerate(token_range.tolist()):
            if self.scale_position == &#34;include&#34;:
                attention_mask[bi, head_idx, :, ti:tj] += scale_constant
            else:
                attention_mask[bi, head_idx, :, :ti] += scale_constant
                attention_mask[bi, head_idx, :, tj:input_len] += scale_constant
        
        if self.model_name == &#34;llama&#34;:
            attention_mask.old_size = attention_mask.size 
            attention_mask.size = lambda:(bsz, 1, tgt_len, src_len)
        
        if &#34;attention_mask&#34; in input_kwargs:
            input_kwargs[&#39;attention_mask&#39;] = attention_mask 
            return input_args, input_kwargs
        else:
            return (input_args[:arg_idx], attention_mask, *input_args[arg_idx+1:]), input_kwargs


    @contextmanager
    def apply_steering(
        self, 
        model: Model, 
        strings: list, 
        substrings: list, 
        model_input: ModelInput, 
        offsets_mapping: Sequence[TokenizerOffsetMapping], 
    ):
        token_range = self.token_ranges_from_batch(
            strings, substrings, offsets_mapping
        )

        registered_hooks = []
        for layer_idx in self.all_layers_idx:
            name = self.ATTN_MODULE_NAME[self.model_name].format(layer_idx)
            module = model.get_submodule(name)
            hook_func = partial(
                self.edit_attention_mask, 
                head_idx = self.head_config[layer_idx],
                token_range = token_range, 
                input_len = model_input[&#39;input_ids&#39;].size(-1)
            )
            registered_hook = module.register_forward_pre_hook(hook_func, with_kwargs=True)
            registered_hooks.append(registered_hook)
        try:
            yield model
        except Exception as error:
            raise error
        finally:
            for registered_hook in registered_hooks:
                registered_hook.remove()
    

    def inputs_from_batch(
        self, 
        text: str | StrSequence,
        tokenizer: Tokenizer|None = None,
        device: Optional[Device] = None,
    ) -&gt; tuple[ModelInput, Sequence[TokenizerOffsetMapping]]:
        &#34;&#34;&#34;Precompute model inputs.&#34;&#34;&#34;
        if tokenizer is None:
            tokenizer = self.tokenizer
        with tokenizer_utils.set_padding_side(tokenizer, padding_side=&#34;left&#34;):
            inputs = tokenizer(
                text,
                return_tensors=&#34;pt&#34;,
                truncation=True,
                padding=&#34;longest&#34;,
                return_offsets_mapping=True,
            )
            offset_mapping = inputs.pop(&#34;offset_mapping&#34;)
        if device is not None:
            inputs = inputs.to(device)
        return inputs, offset_mapping

    @classmethod
    def load_head_config(cls, file:str|Path):
        with open(file, &#34;r&#34;) as f:
            head_config = json.load(f)
        return head_config </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pastalib.pasta.PASTA"><code class="flex name class">
<span>class <span class="ident">PASTA</span></span>
<span>(</span><span>model: transformers.models.gptj.modeling_gptj.GPTJForCausalLM | transformers.models.llama.modeling_llama.LlamaForCausalLM | transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel | transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM, tokenizer: transformers.tokenization_utils_fast.PreTrainedTokenizerFast, head_config: dict | list | None = None, alpha: float = 0.01, scale_position: str = 'exclude')</span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PASTA(abc.ABC):

    ATTN_MODULE_NAME = {
        &#34;gptj&#34;: &#34;transformer.h.{}.attn&#34;,
        &#34;llama&#34;: &#34;model.layers.{}.self_attn&#34;,
    }
    ATTENTION_MASK_ARGIDX = {
        &#34;gptj&#34;: 2, 
        &#34;llama&#34;: 1, 
    }

    def __init__(
        self, 
        model: Model, 
        tokenizer: Tokenizer, 
        head_config: dict|list|None = None, 
        alpha: float = 0.01, 
        scale_position: str = &#34;exclude&#34;, 
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.setup_model(model)

        self.alpha = alpha
        self.scale_position = scale_position
        self.setup_head_config(head_config)

        assert self.scale_position in [&#39;include&#39;, &#39;exclude&#39;]
        assert self.alpha &gt; 0

    def setup_model(self, model):
        if isinstance(model, transformers.LlamaForCausalLM):
            self.model_name = &#34;llama&#34;
            self.num_attn_head = model.config.num_attention_heads
        elif isinstance(model, transformers.GPTJForCausalLM):
            self.model_name = &#34;gptj&#34;
            self.num_attn_head = model.config.n_head
        else:
            raise ValueError(&#34;Unimplemented Model Type.&#34;)

    def setup_head_config(self, head_config):
        if isinstance(head_config, dict):
            self.head_config = {int(k):v for k,v in head_config.items()} 
            self.all_layers_idx = [int(key) for key in head_config]
        elif isinstance(head_config, list):
            self.all_layers_idx = [int(v) for v in head_config]
            self.head_config = {
                idx:list(range(self.num_attn_head)) for idx in self.all_layers_idx
            }
        else:
            raise ValueError(f&#34;Incorrect head config: {head_config}&#34;)
    
    def _maybe_batch(self, text: str | StrSequence) -&gt; StrSequence:
        &#34;&#34;&#34;Batch the text if it is not already batched.&#34;&#34;&#34;
        if isinstance(text, str):
            return [text]
        return text

    def token_ranges_from_batch(
        self,
        strings: str | StrSequence,
        substrings: str | StrSequence,
        offsets_mapping: Sequence[TokenizerOffsetMapping],
        occurrence: int = 0,
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;Return shape (batch_size, 2) tensor of token ranges for (str, substr) pairs.&#34;&#34;&#34;
        strings = self._maybe_batch(strings)
        substrings = self._maybe_batch(substrings)
        if len(strings) != len(substrings):
            raise ValueError(
                f&#34;got {len(strings)} strings but only {len(substrings)} substrings&#34;
            )
        return torch.tensor(
            [
                tokenizer_utils.find_token_range(
                    string, substring, offset_mapping=offset_mapping, occurrence=occurrence
                )
                for string, substring, offset_mapping in zip(
                    strings, substrings, offsets_mapping
                )
            ]
        )

    def edit_attention_mask(
        self, 
        module: torch.nn.Module, 
        input_args: tuple,
        input_kwargs: dict, 
        head_idx: list[int],
        token_range: torch.Tensor, 
        input_len: int, 
    ):
        if &#34;attention_mask&#34; in input_kwargs:
            attention_mask = input_kwargs[&#39;attention_mask&#39;].clone()
        elif input_args is not None:
            arg_idx = self.ATTENTION_MASK_ARGIDX[self.model_name]
            attention_mask = input_args[arg_idx].clone()
        else:
            raise ValueError(f&#34;Not found attention masks in {str(module)}&#34;)
        
        bsz, head_dim, tgt_len, src_len = attention_mask.size()
        dtype, device = attention_mask.dtype, attention_mask.device
        if head_dim != self.num_attn_head:
            attention_mask = attention_mask.expand(
                bsz, self.num_attn_head, tgt_len, src_len
            ).clone()
        scale_constant = torch.Tensor([self.alpha]).to(dtype).to(device).log()
        
        for bi, (ti,tj) in enumerate(token_range.tolist()):
            if self.scale_position == &#34;include&#34;:
                attention_mask[bi, head_idx, :, ti:tj] += scale_constant
            else:
                attention_mask[bi, head_idx, :, :ti] += scale_constant
                attention_mask[bi, head_idx, :, tj:input_len] += scale_constant
        
        if self.model_name == &#34;llama&#34;:
            attention_mask.old_size = attention_mask.size 
            attention_mask.size = lambda:(bsz, 1, tgt_len, src_len)
        
        if &#34;attention_mask&#34; in input_kwargs:
            input_kwargs[&#39;attention_mask&#39;] = attention_mask 
            return input_args, input_kwargs
        else:
            return (input_args[:arg_idx], attention_mask, *input_args[arg_idx+1:]), input_kwargs


    @contextmanager
    def apply_steering(
        self, 
        model: Model, 
        strings: list, 
        substrings: list, 
        model_input: ModelInput, 
        offsets_mapping: Sequence[TokenizerOffsetMapping], 
    ):
        token_range = self.token_ranges_from_batch(
            strings, substrings, offsets_mapping
        )

        registered_hooks = []
        for layer_idx in self.all_layers_idx:
            name = self.ATTN_MODULE_NAME[self.model_name].format(layer_idx)
            module = model.get_submodule(name)
            hook_func = partial(
                self.edit_attention_mask, 
                head_idx = self.head_config[layer_idx],
                token_range = token_range, 
                input_len = model_input[&#39;input_ids&#39;].size(-1)
            )
            registered_hook = module.register_forward_pre_hook(hook_func, with_kwargs=True)
            registered_hooks.append(registered_hook)
        try:
            yield model
        except Exception as error:
            raise error
        finally:
            for registered_hook in registered_hooks:
                registered_hook.remove()
    

    def inputs_from_batch(
        self, 
        text: str | StrSequence,
        tokenizer: Tokenizer|None = None,
        device: Optional[Device] = None,
    ) -&gt; tuple[ModelInput, Sequence[TokenizerOffsetMapping]]:
        &#34;&#34;&#34;Precompute model inputs.&#34;&#34;&#34;
        if tokenizer is None:
            tokenizer = self.tokenizer
        with tokenizer_utils.set_padding_side(tokenizer, padding_side=&#34;left&#34;):
            inputs = tokenizer(
                text,
                return_tensors=&#34;pt&#34;,
                truncation=True,
                padding=&#34;longest&#34;,
                return_offsets_mapping=True,
            )
            offset_mapping = inputs.pop(&#34;offset_mapping&#34;)
        if device is not None:
            inputs = inputs.to(device)
        return inputs, offset_mapping

    @classmethod
    def load_head_config(cls, file:str|Path):
        with open(file, &#34;r&#34;) as f:
            head_config = json.load(f)
        return head_config </code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="pastalib.pasta.PASTA.ATTENTION_MASK_ARGIDX"><code class="name">var <span class="ident">ATTENTION_MASK_ARGIDX</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="pastalib.pasta.PASTA.ATTN_MODULE_NAME"><code class="name">var <span class="ident">ATTN_MODULE_NAME</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="pastalib.pasta.PASTA.load_head_config"><code class="name flex">
<span>def <span class="ident">load_head_config</span></span>(<span>file: str | pathlib.Path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def load_head_config(cls, file:str|Path):
    with open(file, &#34;r&#34;) as f:
        head_config = json.load(f)
    return head_config </code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pastalib.pasta.PASTA.apply_steering"><code class="name flex">
<span>def <span class="ident">apply_steering</span></span>(<span>self, model: transformers.models.gptj.modeling_gptj.GPTJForCausalLM | transformers.models.llama.modeling_llama.LlamaForCausalLM | transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel | transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM, strings: list, substrings: list, model_input: transformers.tokenization_utils_base.BatchEncoding, offsets_mapping: Sequence[Sequence[tuple[int, int]]])</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@contextmanager
def apply_steering(
    self, 
    model: Model, 
    strings: list, 
    substrings: list, 
    model_input: ModelInput, 
    offsets_mapping: Sequence[TokenizerOffsetMapping], 
):
    token_range = self.token_ranges_from_batch(
        strings, substrings, offsets_mapping
    )

    registered_hooks = []
    for layer_idx in self.all_layers_idx:
        name = self.ATTN_MODULE_NAME[self.model_name].format(layer_idx)
        module = model.get_submodule(name)
        hook_func = partial(
            self.edit_attention_mask, 
            head_idx = self.head_config[layer_idx],
            token_range = token_range, 
            input_len = model_input[&#39;input_ids&#39;].size(-1)
        )
        registered_hook = module.register_forward_pre_hook(hook_func, with_kwargs=True)
        registered_hooks.append(registered_hook)
    try:
        yield model
    except Exception as error:
        raise error
    finally:
        for registered_hook in registered_hooks:
            registered_hook.remove()</code></pre>
</details>
</dd>
<dt id="pastalib.pasta.PASTA.edit_attention_mask"><code class="name flex">
<span>def <span class="ident">edit_attention_mask</span></span>(<span>self, module: torch.nn.modules.module.Module, input_args: tuple, input_kwargs: dict, head_idx: list[int], token_range: torch.Tensor, input_len: int)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def edit_attention_mask(
    self, 
    module: torch.nn.Module, 
    input_args: tuple,
    input_kwargs: dict, 
    head_idx: list[int],
    token_range: torch.Tensor, 
    input_len: int, 
):
    if &#34;attention_mask&#34; in input_kwargs:
        attention_mask = input_kwargs[&#39;attention_mask&#39;].clone()
    elif input_args is not None:
        arg_idx = self.ATTENTION_MASK_ARGIDX[self.model_name]
        attention_mask = input_args[arg_idx].clone()
    else:
        raise ValueError(f&#34;Not found attention masks in {str(module)}&#34;)
    
    bsz, head_dim, tgt_len, src_len = attention_mask.size()
    dtype, device = attention_mask.dtype, attention_mask.device
    if head_dim != self.num_attn_head:
        attention_mask = attention_mask.expand(
            bsz, self.num_attn_head, tgt_len, src_len
        ).clone()
    scale_constant = torch.Tensor([self.alpha]).to(dtype).to(device).log()
    
    for bi, (ti,tj) in enumerate(token_range.tolist()):
        if self.scale_position == &#34;include&#34;:
            attention_mask[bi, head_idx, :, ti:tj] += scale_constant
        else:
            attention_mask[bi, head_idx, :, :ti] += scale_constant
            attention_mask[bi, head_idx, :, tj:input_len] += scale_constant
    
    if self.model_name == &#34;llama&#34;:
        attention_mask.old_size = attention_mask.size 
        attention_mask.size = lambda:(bsz, 1, tgt_len, src_len)
    
    if &#34;attention_mask&#34; in input_kwargs:
        input_kwargs[&#39;attention_mask&#39;] = attention_mask 
        return input_args, input_kwargs
    else:
        return (input_args[:arg_idx], attention_mask, *input_args[arg_idx+1:]), input_kwargs</code></pre>
</details>
</dd>
<dt id="pastalib.pasta.PASTA.inputs_from_batch"><code class="name flex">
<span>def <span class="ident">inputs_from_batch</span></span>(<span>self, text: str | list[str] | tuple[str, ...], tokenizer: transformers.tokenization_utils_fast.PreTrainedTokenizerFast | None = None, device: Union[str, torch.device, ForwardRef(None)] = None) ‑> tuple[transformers.tokenization_utils_base.BatchEncoding, typing.Sequence[typing.Sequence[tuple[int, int]]]]</span>
</code></dt>
<dd>
<div class="desc"><p>Precompute model inputs.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inputs_from_batch(
    self, 
    text: str | StrSequence,
    tokenizer: Tokenizer|None = None,
    device: Optional[Device] = None,
) -&gt; tuple[ModelInput, Sequence[TokenizerOffsetMapping]]:
    &#34;&#34;&#34;Precompute model inputs.&#34;&#34;&#34;
    if tokenizer is None:
        tokenizer = self.tokenizer
    with tokenizer_utils.set_padding_side(tokenizer, padding_side=&#34;left&#34;):
        inputs = tokenizer(
            text,
            return_tensors=&#34;pt&#34;,
            truncation=True,
            padding=&#34;longest&#34;,
            return_offsets_mapping=True,
        )
        offset_mapping = inputs.pop(&#34;offset_mapping&#34;)
    if device is not None:
        inputs = inputs.to(device)
    return inputs, offset_mapping</code></pre>
</details>
</dd>
<dt id="pastalib.pasta.PASTA.setup_head_config"><code class="name flex">
<span>def <span class="ident">setup_head_config</span></span>(<span>self, head_config)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup_head_config(self, head_config):
    if isinstance(head_config, dict):
        self.head_config = {int(k):v for k,v in head_config.items()} 
        self.all_layers_idx = [int(key) for key in head_config]
    elif isinstance(head_config, list):
        self.all_layers_idx = [int(v) for v in head_config]
        self.head_config = {
            idx:list(range(self.num_attn_head)) for idx in self.all_layers_idx
        }
    else:
        raise ValueError(f&#34;Incorrect head config: {head_config}&#34;)</code></pre>
</details>
</dd>
<dt id="pastalib.pasta.PASTA.setup_model"><code class="name flex">
<span>def <span class="ident">setup_model</span></span>(<span>self, model)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup_model(self, model):
    if isinstance(model, transformers.LlamaForCausalLM):
        self.model_name = &#34;llama&#34;
        self.num_attn_head = model.config.num_attention_heads
    elif isinstance(model, transformers.GPTJForCausalLM):
        self.model_name = &#34;gptj&#34;
        self.num_attn_head = model.config.n_head
    else:
        raise ValueError(&#34;Unimplemented Model Type.&#34;)</code></pre>
</details>
</dd>
<dt id="pastalib.pasta.PASTA.token_ranges_from_batch"><code class="name flex">
<span>def <span class="ident">token_ranges_from_batch</span></span>(<span>self, strings: str | list[str] | tuple[str, ...], substrings: str | list[str] | tuple[str, ...], offsets_mapping: Sequence[Sequence[tuple[int, int]]], occurrence: int = 0) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Return shape (batch_size, 2) tensor of token ranges for (str, substr) pairs.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def token_ranges_from_batch(
    self,
    strings: str | StrSequence,
    substrings: str | StrSequence,
    offsets_mapping: Sequence[TokenizerOffsetMapping],
    occurrence: int = 0,
) -&gt; torch.Tensor:
    &#34;&#34;&#34;Return shape (batch_size, 2) tensor of token ranges for (str, substr) pairs.&#34;&#34;&#34;
    strings = self._maybe_batch(strings)
    substrings = self._maybe_batch(substrings)
    if len(strings) != len(substrings):
        raise ValueError(
            f&#34;got {len(strings)} strings but only {len(substrings)} substrings&#34;
        )
    return torch.tensor(
        [
            tokenizer_utils.find_token_range(
                string, substring, offset_mapping=offset_mapping, occurrence=occurrence
            )
            for string, substring, offset_mapping in zip(
                strings, substrings, offsets_mapping
            )
        ]
    )</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pastalib" href="index.html">pastalib</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pastalib.pasta.PASTA" href="#pastalib.pasta.PASTA">PASTA</a></code></h4>
<ul class="">
<li><code><a title="pastalib.pasta.PASTA.ATTENTION_MASK_ARGIDX" href="#pastalib.pasta.PASTA.ATTENTION_MASK_ARGIDX">ATTENTION_MASK_ARGIDX</a></code></li>
<li><code><a title="pastalib.pasta.PASTA.ATTN_MODULE_NAME" href="#pastalib.pasta.PASTA.ATTN_MODULE_NAME">ATTN_MODULE_NAME</a></code></li>
<li><code><a title="pastalib.pasta.PASTA.apply_steering" href="#pastalib.pasta.PASTA.apply_steering">apply_steering</a></code></li>
<li><code><a title="pastalib.pasta.PASTA.edit_attention_mask" href="#pastalib.pasta.PASTA.edit_attention_mask">edit_attention_mask</a></code></li>
<li><code><a title="pastalib.pasta.PASTA.inputs_from_batch" href="#pastalib.pasta.PASTA.inputs_from_batch">inputs_from_batch</a></code></li>
<li><code><a title="pastalib.pasta.PASTA.load_head_config" href="#pastalib.pasta.PASTA.load_head_config">load_head_config</a></code></li>
<li><code><a title="pastalib.pasta.PASTA.setup_head_config" href="#pastalib.pasta.PASTA.setup_head_config">setup_head_config</a></code></li>
<li><code><a title="pastalib.pasta.PASTA.setup_model" href="#pastalib.pasta.PASTA.setup_model">setup_model</a></code></li>
<li><code><a title="pastalib.pasta.PASTA.token_ranges_from_batch" href="#pastalib.pasta.PASTA.token_ranges_from_batch">token_ranges_from_batch</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>