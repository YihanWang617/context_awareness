<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>pastalib.utils.tokenizer_utils API documentation</title>
<meta name="description" content="Utils for interacting with huggingface tokenizers." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pastalib.utils.tokenizer_utils</code></h1>
</header>
<section id="section-intro">
<p>Utils for interacting with huggingface tokenizers.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Utils for interacting with huggingface tokenizers.&#34;&#34;&#34;
from contextlib import contextmanager
from typing import Any, Iterator, Optional, Sequence, Tuple

from .typing import StrSequence, Tokenizer, TokenizerOffsetMapping


def find_token_range(
    string: str,
    substring: str,
    tokenizer: Optional[Tokenizer] = None,
    occurrence: int = 0,
    offset_mapping: Optional[TokenizerOffsetMapping] = None,
    **kwargs: Any,
) -&gt; Tuple[int, int]:
    &#34;&#34;&#34;Find index range of tokenized string containing tokens for substring.

    The kwargs are forwarded to the tokenizer.

    A simple example:

        string = &#39;The batman is the night.&#39;
        substring = &#39;batman&#39;
        tokenizer = ...

        # Example tokenization: [&#39;the&#39;, &#39;bat&#39;, &#39;##man&#39;, &#39;is&#39;, &#39;the&#39;, &#39;night&#39;]
        assert find_token_range(string, substring, tokenizer) == (1, 3)

    Args:
        string: The string.
        substring: The substring to find token range for.
        tokenizer: The tokenizer. If not set, offset_mapping must be.
        occurrence: The occurence of the substring to look for.
            Zero indexed. Defaults to 0, the first occurrence.
        offset_mapping: Precomputed offset mapping. If not set, tokenizer will be run.

    Raises:
        ValueError: If substring is not actually in string or if banned
            kwargs are specified.

    Returns:
        Tuple[int, int]: The start (inclusive) and end (exclusive) token idx.
    &#34;&#34;&#34;
    if tokenizer is None and offset_mapping is None:
        raise ValueError(&#34;must set either tokenizer= or offset_mapping=&#34;)
    if &#34;return_offsets_mapping&#34; in kwargs:
        raise ValueError(&#34;cannot set return_offsets_mapping&#34;)
    if substring not in string:
        raise ValueError(f&#39;&#34;{substring}&#34; not found in &#34;{string}&#34;&#39;)
    char_start = string.index(substring)
    for _ in range(occurrence):
        try:
            char_start = string.index(substring, char_start + 1)
        except ValueError as error:
            raise ValueError(
                f&#34;could not find {occurrence + 1} occurrences &#34;
                f&#39;of &#34;{substring} in &#34;{string}&#34;&#39;
            ) from error
    char_end = char_start + len(substring)

    if offset_mapping is None:
        assert tokenizer is not None
        tokens = tokenizer(string, return_offsets_mapping=True, **kwargs)
        offset_mapping = tokens.offset_mapping

    token_start, token_end = None, None
    for index, (token_char_start, token_char_end) in enumerate(offset_mapping):
        if token_start is None:
            if token_char_start &lt;= char_start and token_char_end &gt;= char_start:
                token_start = index
        if token_end is None:
            if token_char_start &lt;= char_end and token_char_end &gt;= char_end:
                token_end = index
                break

    assert token_start is not None
    assert token_end is not None
    assert token_start &lt;= token_end
    return (token_start, token_end + 1)


def batch_convert_ids_to_tokens(
    batch: Sequence[Sequence[int]], tokenizer: Tokenizer, **kwargs: Any
) -&gt; Sequence[StrSequence]:
    &#34;&#34;&#34;Call `convert_ids_to_tokens` on every sequence in the batch.&#34;&#34;&#34;
    return [tokenizer.convert_ids_to_tokens(ids, **kwargs) for ids in batch]


@contextmanager
def set_padding_side(
    tokenizer: Tokenizer, padding_side: str = &#34;right&#34;
) -&gt; Iterator[None]:
    &#34;&#34;&#34;Temporarily set padding side for tokenizer.

    Useful for when you want to batch generate with causal LMs like GPT, as these
    require the padding to be on the left side in such settings but are much easier
    to mess around with when the padding, by default, is on the right.

    Example usage:
        tokenizer = transformers.AutoTokenizer.from_pretrained(&#34;gpt2&#34;)
        with tokenizer_utils.set_padding_side(tokenizer, &#34;left&#34;):
            inputs = mt.tokenizer(...)
        # ... later
        model.generate(**inputs)

    &#34;&#34;&#34;
    _padding_side = tokenizer.padding_side
    tokenizer.padding_side = padding_side
    yield
    tokenizer.padding_side = _padding_side</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pastalib.utils.tokenizer_utils.batch_convert_ids_to_tokens"><code class="name flex">
<span>def <span class="ident">batch_convert_ids_to_tokens</span></span>(<span>batch: Sequence[Sequence[int]], tokenizer: transformers.tokenization_utils_fast.PreTrainedTokenizerFast, **kwargs: Any) ‑> Sequence[list[str] | tuple[str, ...]]</span>
</code></dt>
<dd>
<div class="desc"><p>Call <code>convert_ids_to_tokens</code> on every sequence in the batch.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def batch_convert_ids_to_tokens(
    batch: Sequence[Sequence[int]], tokenizer: Tokenizer, **kwargs: Any
) -&gt; Sequence[StrSequence]:
    &#34;&#34;&#34;Call `convert_ids_to_tokens` on every sequence in the batch.&#34;&#34;&#34;
    return [tokenizer.convert_ids_to_tokens(ids, **kwargs) for ids in batch]</code></pre>
</details>
</dd>
<dt id="pastalib.utils.tokenizer_utils.find_token_range"><code class="name flex">
<span>def <span class="ident">find_token_range</span></span>(<span>string: str, substring: str, tokenizer: transformers.tokenization_utils_fast.PreTrainedTokenizerFast | None = None, occurrence: int = 0, offset_mapping: Optional[Sequence[tuple[int, int]]] = None, **kwargs: Any) ‑> Tuple[int, int]</span>
</code></dt>
<dd>
<div class="desc"><p>Find index range of tokenized string containing tokens for substring.</p>
<p>The kwargs are forwarded to the tokenizer.</p>
<p>A simple example:</p>
<pre><code>string = 'The batman is the night.'
substring = 'batman'
tokenizer = ...

# Example tokenization: ['the', 'bat', '##man', 'is', 'the', 'night']
assert find_token_range(string, substring, tokenizer) == (1, 3)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>string</code></strong></dt>
<dd>The string.</dd>
<dt><strong><code>substring</code></strong></dt>
<dd>The substring to find token range for.</dd>
<dt><strong><code>tokenizer</code></strong></dt>
<dd>The tokenizer. If not set, offset_mapping must be.</dd>
<dt><strong><code>occurrence</code></strong></dt>
<dd>The occurence of the substring to look for.
Zero indexed. Defaults to 0, the first occurrence.</dd>
<dt><strong><code>offset_mapping</code></strong></dt>
<dd>Precomputed offset mapping. If not set, tokenizer will be run.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If substring is not actually in string or if banned
kwargs are specified.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[int, int]</code></dt>
<dd>The start (inclusive) and end (exclusive) token idx.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_token_range(
    string: str,
    substring: str,
    tokenizer: Optional[Tokenizer] = None,
    occurrence: int = 0,
    offset_mapping: Optional[TokenizerOffsetMapping] = None,
    **kwargs: Any,
) -&gt; Tuple[int, int]:
    &#34;&#34;&#34;Find index range of tokenized string containing tokens for substring.

    The kwargs are forwarded to the tokenizer.

    A simple example:

        string = &#39;The batman is the night.&#39;
        substring = &#39;batman&#39;
        tokenizer = ...

        # Example tokenization: [&#39;the&#39;, &#39;bat&#39;, &#39;##man&#39;, &#39;is&#39;, &#39;the&#39;, &#39;night&#39;]
        assert find_token_range(string, substring, tokenizer) == (1, 3)

    Args:
        string: The string.
        substring: The substring to find token range for.
        tokenizer: The tokenizer. If not set, offset_mapping must be.
        occurrence: The occurence of the substring to look for.
            Zero indexed. Defaults to 0, the first occurrence.
        offset_mapping: Precomputed offset mapping. If not set, tokenizer will be run.

    Raises:
        ValueError: If substring is not actually in string or if banned
            kwargs are specified.

    Returns:
        Tuple[int, int]: The start (inclusive) and end (exclusive) token idx.
    &#34;&#34;&#34;
    if tokenizer is None and offset_mapping is None:
        raise ValueError(&#34;must set either tokenizer= or offset_mapping=&#34;)
    if &#34;return_offsets_mapping&#34; in kwargs:
        raise ValueError(&#34;cannot set return_offsets_mapping&#34;)
    if substring not in string:
        raise ValueError(f&#39;&#34;{substring}&#34; not found in &#34;{string}&#34;&#39;)
    char_start = string.index(substring)
    for _ in range(occurrence):
        try:
            char_start = string.index(substring, char_start + 1)
        except ValueError as error:
            raise ValueError(
                f&#34;could not find {occurrence + 1} occurrences &#34;
                f&#39;of &#34;{substring} in &#34;{string}&#34;&#39;
            ) from error
    char_end = char_start + len(substring)

    if offset_mapping is None:
        assert tokenizer is not None
        tokens = tokenizer(string, return_offsets_mapping=True, **kwargs)
        offset_mapping = tokens.offset_mapping

    token_start, token_end = None, None
    for index, (token_char_start, token_char_end) in enumerate(offset_mapping):
        if token_start is None:
            if token_char_start &lt;= char_start and token_char_end &gt;= char_start:
                token_start = index
        if token_end is None:
            if token_char_start &lt;= char_end and token_char_end &gt;= char_end:
                token_end = index
                break

    assert token_start is not None
    assert token_end is not None
    assert token_start &lt;= token_end
    return (token_start, token_end + 1)</code></pre>
</details>
</dd>
<dt id="pastalib.utils.tokenizer_utils.set_padding_side"><code class="name flex">
<span>def <span class="ident">set_padding_side</span></span>(<span>tokenizer: transformers.tokenization_utils_fast.PreTrainedTokenizerFast, padding_side: str = 'right') ‑> Iterator[None]</span>
</code></dt>
<dd>
<div class="desc"><p>Temporarily set padding side for tokenizer.</p>
<p>Useful for when you want to batch generate with causal LMs like GPT, as these
require the padding to be on the left side in such settings but are much easier
to mess around with when the padding, by default, is on the right.</p>
<p>Example usage:
tokenizer = transformers.AutoTokenizer.from_pretrained("gpt2")
with tokenizer_utils.set_padding_side(tokenizer, "left"):
inputs = mt.tokenizer(&hellip;)
# &hellip; later
model.generate(**inputs)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@contextmanager
def set_padding_side(
    tokenizer: Tokenizer, padding_side: str = &#34;right&#34;
) -&gt; Iterator[None]:
    &#34;&#34;&#34;Temporarily set padding side for tokenizer.

    Useful for when you want to batch generate with causal LMs like GPT, as these
    require the padding to be on the left side in such settings but are much easier
    to mess around with when the padding, by default, is on the right.

    Example usage:
        tokenizer = transformers.AutoTokenizer.from_pretrained(&#34;gpt2&#34;)
        with tokenizer_utils.set_padding_side(tokenizer, &#34;left&#34;):
            inputs = mt.tokenizer(...)
        # ... later
        model.generate(**inputs)

    &#34;&#34;&#34;
    _padding_side = tokenizer.padding_side
    tokenizer.padding_side = padding_side
    yield
    tokenizer.padding_side = _padding_side</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pastalib.utils" href="index.html">pastalib.utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="pastalib.utils.tokenizer_utils.batch_convert_ids_to_tokens" href="#pastalib.utils.tokenizer_utils.batch_convert_ids_to_tokens">batch_convert_ids_to_tokens</a></code></li>
<li><code><a title="pastalib.utils.tokenizer_utils.find_token_range" href="#pastalib.utils.tokenizer_utils.find_token_range">find_token_range</a></code></li>
<li><code><a title="pastalib.utils.tokenizer_utils.set_padding_side" href="#pastalib.utils.tokenizer_utils.set_padding_side">set_padding_side</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>